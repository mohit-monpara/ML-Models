{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "# !pip install pydot\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Flatten, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from plotly.offline import iplot\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the files from the folder\n",
    "\n",
    "fake_news_df = pd.read_csv('Fake.csv')\n",
    "true_news_df = pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the random instances of the data\n",
    "\n",
    "display(fake_news_df.sample(2), true_news_df.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning  ###\n",
    "\n",
    "# Remove the HTML text/phases from the data\n",
    "def remove_html(text):\n",
    "    new_text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", text)\n",
    "    return(new_text)\n",
    "\n",
    "# Count the lenght of the string\n",
    "def len_text(text):\n",
    "    text_len = len(text.split())\n",
    "    return(text_len)\n",
    "    \n",
    "# Remove White Spaces\n",
    "def remove_white_space(text):\n",
    "    text = re.sub(\"^\\s+|\\s+$\", \"\", text, flags=re.UNICODE) # Remove spaces both in beginining and in the end of a string\n",
    "    text = \" \".join(re.split(\"\\s+\", text, flags=re.UNICODE)) # Remove spaces from duplicate spaces\n",
    "    return(text)\n",
    "\n",
    "# Removing the Accented Chars\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "# Removing Special characters\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-z0-9\\s.]' \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis (Before Data Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fake_news_df.sample(2), true_news_df.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fake_news_df.shape, true_news_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAKE News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(set(fake_news_df['subject']), set(true_news_df['subject']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of each instance\n",
    "fake_news_df['len_sent'] = fake_news_df['text'].apply(lambda x: len_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_df['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the description of each group in the subject feature\n",
    "fake_news_df.groupby(['subject']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(123)\n",
    "# fake_news_df[fake_news_df['subject']=='politics']['text'].sample(2, random_state=123).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion for Statistical analysis on Fake News Dataframe\n",
    "\n",
    "- Maximum instances of possess by \"News\" with 9050 instances, followed by \"politics\" [6841], \"left-news\" [4459], \"Govt News\" [1570], \"US_news\" [783], and \"Middle-east\" [778]\n",
    "- The news corpus is largely aligned towards the \"left-news\" and \"politics\" \n",
    "- The minimum number of text in Middle-east, News and US_News starts from 24, 36 and 24, respectively.\n",
    "- There are various instances in Govt News, left-news and politics where the news is empty\n",
    "- Many dirty records can be found in the data i.e. \n",
    "    - HTML characters/Code\n",
    "    - White Spaces in the text\n",
    "    - Removing Ascented Characters\n",
    "    - Removing Special Characters\n",
    "\n",
    "##### Cleaning Aspects\n",
    "\n",
    "- Remove the instances with length less than 10\n",
    "- Split the total instances by each group in same proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRUE News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(set(fake_news_df['subject']), set(true_news_df['subject']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of each instance\n",
    "true_news_df['len_sent'] = true_news_df['text'].apply(lambda x: len_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news_df['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the description of each group in the subject feature\n",
    "true_news_df.groupby(['subject']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion for Statistical analysis on True News Dataframe\n",
    "\n",
    "- Maximum instances of possess by \"politicsNews\" with 11272 instances, followed by \"worldnews\" [10145]\n",
    "- There are various instances in \"politicsNews\" where the news is empty\n",
    "\n",
    "##### Cleaning Aspects\n",
    "\n",
    "- Remove the instances with length less than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Conclusion\n",
    "- Set minimum length of the instances to 20\n",
    "- Make sure to remove bais in the data i.e. all group should possess same amount of instances (data)\n",
    "- Combine the fields (if required) i.e. combining the \"subjects\" of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis (After Data Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAKE News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_df['text'] = fake_news_df['text'].apply(lambda x: remove_html(x))\n",
    "fake_news_df['text'] = fake_news_df['text'].apply(lambda x: remove_accented_chars(x))\n",
    "fake_news_df['text'] = fake_news_df['text'].apply(lambda x: remove_special_characters(x))\n",
    "fake_news_df['text'] = fake_news_df['text'].apply(lambda x: remove_white_space(x))\n",
    "fake_news_df['len_sent'] = fake_news_df['text'].apply(lambda x: len_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of sentences below 10 word length\n",
    "fake_news_df[fake_news_df['len_sent'] < 21].sort_values(by=\"len_sent\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_df = fake_news_df[fake_news_df['len_sent'] > 20].reset_index(drop=True)\n",
    "display(fake_news_df.shape, fake_news_df.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_df['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the description of each group in the subject feature\n",
    "fake_news_df.groupby(['subject']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding real_fake feature in order to identify the True or Fake News\n",
    "fake_news_df['real_or_fake'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Fake News: \", fake_news_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRUE News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news_df['text'] = true_news_df['text'].apply(lambda x: remove_html(x))\n",
    "true_news_df['text'] = true_news_df['text'].apply(lambda x: remove_accented_chars(x))\n",
    "true_news_df['text'] = true_news_df['text'].apply(lambda x: remove_special_characters(x))\n",
    "true_news_df['text'] = true_news_df['text'].apply(lambda x: remove_white_space(x))\n",
    "true_news_df['len_sent'] = true_news_df['text'].apply(lambda x: len_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of sentences below 10 word length\n",
    "true_news_df[true_news_df['len_sent'] < 21].sort_values(by=\"len_sent\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the description of each group in the subject feature\n",
    "true_news_df.groupby(['subject']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news_df = true_news_df[true_news_df['len_sent'] > 20].reset_index(drop=True)\n",
    "display(true_news_df.shape, true_news_df.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news_df['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the description of each group in the subject feature\n",
    "true_news_df.groupby(['subject']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding real_fake feature in order to identify the True or Fake News\n",
    "true_news_df['real_or_fake'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of True News: \", true_news_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have same number of news instances in both the dataframe\n",
    "fake_news_df = fake_news_df.sample(true_news_df.shape[0]).reset_index(drop=True)\n",
    "print(\"Shape of Fake News: \", fake_news_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Section - Pre-Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([fake_news_df,true_news_df])\n",
    "final_df = final_df.sample(frac=1).reset_index(drop=True)\n",
    "display(final_df.shape, final_df.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting text-information into list\n",
    "news_text = final_df['text'].to_list()\n",
    "len(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating One-Hot encoding for the 'real_or_fake' feature\n",
    "le = LabelEncoder()\n",
    "oe = OneHotEncoder(sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_fake = oe.fit_transform(final_df.real_or_fake.values.reshape(-1,1))\n",
    "real_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer =Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(final_df['text'].values)  #  to update the internal vocabulary for the texts list\n",
    "word_index = tokenizer.word_index \n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Word Index Sample: \", word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word Index](Image/word_index.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = tokenizer.texts_to_sequences(final_df['text'].values) # converting tokens of text corpus into a sequence of integers\n",
    "news_text = pad_sequences(news_text, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', news_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(final_df['real_or_fake']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(news_text, Y, test_size=0.2, random_state=100)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf-od')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaf0c9af9c15e2b81ba96803a9326656c5add24c3ac6d6b00ab7ba7f31f2c52d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
